{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers faiss-cpu googlesearch-python beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from googlesearch import search\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell laden (multilingual, inkl. Deutsch)\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text_from_url(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        paragraphs = soup.find_all(['p'])\n",
    "        text = ' '.join(p.get_text() for p in paragraphs)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text if len(text) > 100 else ''\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def film_suche(query, num_results=10, top_k=5):\n",
    "    print(\"Suche im Webâ€¦\")\n",
    "    # deutschsprachige Filmseiten durchsuchen\n",
    "    sites = [\"de.wikipedia.org\", \"filmstarts.de\", \"moviepilot.de\"]\n",
    "    query_sites = \" OR \".join(f\"site:{s}\" for s in sites)\n",
    "    urls = list(search(f\"{query} {query_sites}\", num_results=num_results, lang='de'))\n",
    "    print(f\"{len(urls)} URLs gefunden.\")\n",
    "\n",
    "    docs, sources = [], []\n",
    "    for url in urls:\n",
    "        text = scrape_text_from_url(url)\n",
    "        if text:\n",
    "            docs.append(text)\n",
    "            sources.append(url)\n",
    "    print(f\"{len(docs)} Texte extrahiert.\")\n",
    "    if not docs:\n",
    "        print(\"Keine Inhalte gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Embeddings & FAISS\n",
    "    doc_emb = model.encode(docs, convert_to_numpy=True)\n",
    "    index = faiss.IndexFlatL2(doc_emb.shape[1])\n",
    "    index.add(doc_emb)\n",
    "\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "\n",
    "    print(\"\\nTop-Ergebnisse:\")\n",
    "    for idx in I[0]:\n",
    "        print(\"---\")\n",
    "        print(f\"Quelle: {sources[idx]}\")\n",
    "        print(docs[idx][:500] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Aufruf:\n",
    "film_suche(\"Film mit Zeitreise und Motorrad in den 80ern\", num_results=15, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
